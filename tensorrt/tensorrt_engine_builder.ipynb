{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the nvidia TensorRT LLM repository\n",
    "!git clone https://github.com/NVIDIA/TensorRT-LLM.git\n",
    "%cd TensorRT-LLM/examples/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if tensorrt_llm is correctly installed\n",
    "import tensorrt_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens for gated models will be fetched through modal secrets\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "snapshot_download(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    local_dir=\"tmp/hf_models/Meta-Llama-3-8B-Instruct\",\n",
    "    max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the raw model weights into tensorrt-llm checkpoint format\n",
    "\n",
    "!python convert_checkpoint.py --model_dir ./tmp/hf_models/Meta-Llama-3-8B-Instruct \\\n",
    "                             --output_dir ./tmp/trt_engines/1-gpu/ \\\n",
    "                             --dtype float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, refere NVIDIA repo for info abouts arguments\n",
    "\n",
    "!trtllm-build --checkpoint_dir ./tmp/trt_engines/1-gpu/ \\\n",
    "            --output_dir ./tmp/trt_engines/compiled-model/ \\\n",
    "            --gpt_attention_plugin float16 \\\n",
    "            --gemm_plugin float16 \\\n",
    "            --max_input_len 32256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the compiled model to hugging face hub\n",
    "\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "for root, dirs, files in os.walk(f\"tmp/trt_engines/compiled-model\", topdown=False):\n",
    "    for name in files:\n",
    "        filepath = os.path.join(root, name)\n",
    "        filename = \"/\".join(filepath.split(\"/\")[-2:])\n",
    "        print(\"uploading file: \", filename)\n",
    "        api = HfApi(token= os.environ[\"HF_WRITE_TOKEN\"])\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=filepath,\n",
    "            path_in_repo=filename,\n",
    "            repo_id=\"agyaatcoder/llama3-8b-instruct-A100-trtllm\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the compiled model\n",
    "\n",
    "!python3 run.py --max_output_len=256 \\\n",
    "               --tokenizer_dir ./llama/tmp/hf_models/llama3-8b-instruct-A100-trtllm/ \\\n",
    "               --engine_dir=./llama/tmp/trt_engines/compiled-model \\\n",
    "               --max_attention_window_size=4096"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
